{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "name": "Resume_Screening_Subhajit.ipynb",
      "provenance": [],
      "collapsed_sections": [],
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/subhajitchou/RESUME_SCREENING_NLP/blob/main/Resume_Screening_Subhajit.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "!pip install pdfminer.six"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "HAYAI1tbGaPP",
        "outputId": "492a2780-9e0b-4104-c8dd-96c5d538bbbd"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Collecting pdfminer.six\n",
            "  Downloading pdfminer.six-20211012-py3-none-any.whl (5.6 MB)\n",
            "\u001b[K     |████████████████████████████████| 5.6 MB 14.1 MB/s \n",
            "\u001b[?25hRequirement already satisfied: chardet in /usr/local/lib/python3.7/dist-packages (from pdfminer.six) (3.0.4)\n",
            "Collecting cryptography\n",
            "  Downloading cryptography-36.0.1-cp36-abi3-manylinux_2_24_x86_64.whl (3.6 MB)\n",
            "\u001b[K     |████████████████████████████████| 3.6 MB 45.9 MB/s \n",
            "\u001b[?25hRequirement already satisfied: cffi>=1.12 in /usr/local/lib/python3.7/dist-packages (from cryptography->pdfminer.six) (1.15.0)\n",
            "Requirement already satisfied: pycparser in /usr/local/lib/python3.7/dist-packages (from cffi>=1.12->cryptography->pdfminer.six) (2.21)\n",
            "Installing collected packages: cryptography, pdfminer.six\n",
            "Successfully installed cryptography-36.0.1 pdfminer.six-20211012\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "### Import python modules and main pdf reader module pdfminer "
      ],
      "metadata": {
        "id": "20DaKdXTHfZP"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import nltk\n",
        "import numpy as np\n",
        "from pdfminer.high_level import extract_text\n",
        "import re\n",
        "import subprocess\n",
        "import pandas as pd\n",
        "nltk.download('stopwords')"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "g7fnc3V_Esld",
        "outputId": "27c052ee-3208-4121-c11f-87e4cd96cfe3"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "[nltk_data] Downloading package stopwords to /root/nltk_data...\n",
            "[nltk_data]   Unzipping corpora/stopwords.zip.\n"
          ]
        },
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "True"
            ]
          },
          "metadata": {},
          "execution_count": 2
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "### Connect to google drive to access resumes "
      ],
      "metadata": {
        "id": "D9FgEwCQHenW"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "from google.colab import drive\n",
        "drive.mount('/content/drive/')"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "cIqd9eIuBcp0",
        "outputId": "de4a3074-3f1c-43f1-8e62-cec46193dc03"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Mounted at /content/drive/\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "resume_path = '/content/drive/MyDrive/AI/Resume'"
      ],
      "metadata": {
        "id": "tVUWQKdJCebP"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "### Define Function to read txt from PDF "
      ],
      "metadata": {
        "id": "zaJ8sKUTHcxW"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "nltk.download('punkt')\n",
        "nltk.download('averaged_perceptron_tagger')\n",
        "nltk.download('maxent_ne_chunker')\n",
        "nltk.download('words')\n",
        "\n",
        "\n",
        "def text_extract_from_pdf(pdf_path):\n",
        "    return extract_text(pdf_path)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "tq2TNskHIVg3",
        "outputId": "22d5d6e7-3da6-4c6c-c693-bdaca1ae04d3"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "[nltk_data] Downloading package punkt to /root/nltk_data...\n",
            "[nltk_data]   Unzipping tokenizers/punkt.zip.\n",
            "[nltk_data] Downloading package averaged_perceptron_tagger to\n",
            "[nltk_data]     /root/nltk_data...\n",
            "[nltk_data]   Unzipping taggers/averaged_perceptron_tagger.zip.\n",
            "[nltk_data] Downloading package maxent_ne_chunker to\n",
            "[nltk_data]     /root/nltk_data...\n",
            "[nltk_data]   Unzipping chunkers/maxent_ne_chunker.zip.\n",
            "[nltk_data] Downloading package words to /root/nltk_data...\n",
            "[nltk_data]   Unzipping corpora/words.zip.\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "### Define function to read names from the text extracted from PDF documents "
      ],
      "metadata": {
        "id": "wMCu4kIoIZVe"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "def extract_name(txt):\n",
        "    candidate_name = []\n",
        "\n",
        "    for sent in nltk.sent_tokenize(txt):\n",
        "        for chunk in nltk.ne_chunk(nltk.pos_tag(nltk.word_tokenize(sent))):\n",
        "            if hasattr(chunk, 'label') and chunk.label() == 'CANDIDATE':\n",
        "                candidate_name.append(\n",
        "                    ' '.join(chunk_leave[0] for chunk_leave in chunk.leaves())\n",
        "                )\n",
        "\n",
        "    return candidate_name"
      ],
      "metadata": {
        "id": "2GfUl71qEoL8"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "### Define function to read Phone-Numbers from the text extracted from PDF documents"
      ],
      "metadata": {
        "id": "Q_8vcuaUJ3yv"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "PHONE_NUM = re.compile(r'[\\+\\(]?[1-9][0-9 .\\-\\(\\)]{8,}[0-9]')\n",
        "\n",
        "def phone_number_extract(resume_text):\n",
        "    phone = re.findall(PHONE_NUM, resume_text)\n",
        "\n",
        "    if phone:\n",
        "        number = ''.join(phone[0])\n",
        "\n",
        "        if resume_text.find(number) >= 0 and len(number) < 16:\n",
        "            return number\n",
        "    return None"
      ],
      "metadata": {
        "id": "CgovQ2cSKUYQ"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "### Define function to read Email from the text extracted from PDF documents"
      ],
      "metadata": {
        "id": "igMeU0RqLIoH"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "EMAIL_ID = re.compile(r'[a-z0-9\\.\\-+_]+@[a-z0-9\\.\\-+_]+\\.[a-z]+')\n",
        "\n",
        "def extract_email(resume_text):\n",
        "    return re.findall(EMAIL_ID, resume_text)"
      ],
      "metadata": {
        "id": "Es9u37p9LCI_"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "### **Function to print name, Phone Number, email from resume**"
      ],
      "metadata": {
        "id": "ePCvs9zeImxX"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "if __name__ == '__main__':\n",
        "    text = text_extract_from_pdf('/content/gdrive/MyDrive/Resume_Screening/Ankit_Gupta_Resume.pdf')\n",
        "    #print(text)\n",
        "    names = extract_name(text)\n",
        "    phone_number = phone_number_extract(text)\n",
        "    emails = extract_email(text)\n",
        "    if names:\n",
        "        print(\"Name:\",names[0])\n",
        "    if phone_number:\n",
        "        print(\"Phone Number:\",phone_number)\n",
        "    if emails:\n",
        "        print(\"Email Address:\",emails[0])"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "FMXfHW8YIjLP",
        "outputId": "e9666a23-4391-41c6-ae3b-0958eb342486"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Name: Gupta Indian Institute\n",
            "Phone Number: 8292858632\n",
            "Email Address: ankitgupta8292@gmail.com\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "## **Extract Skills**"
      ],
      "metadata": {
        "id": "BqUNNOisMKOp"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "### Define function to get skills and put them in technical/ non-technical based on csv provided "
      ],
      "metadata": {
        "id": "D2oiLVNrM0QY"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Read the csv files and prepare Technical & Non-Technical Groups\n",
        "\n",
        "Tech_Skill_Group = []\n",
        "Non_Tech_Skill_Group = []\n",
        "\n",
        "skill_nontech = pd.read_csv('/content/drive/MyDrive/AI/Resume/nontechnicalskills.csv',header=None)\n",
        "skill_tech = pd.read_csv('/content/drive/MyDrive/AI/Resume/techskill.csv',header=None)"
      ],
      "metadata": {
        "id": "MBuCf9XTRyjE"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "Technical_Skills_DB = nltk.flatten(skill_tech.values.tolist())\n",
        "\n",
        "Non_Tech_Skill_Group = nltk.flatten(skill_nontech.values.tolist())\n",
        "print (Non_Tech_Skill_Group)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "T6SOxKFbSFlT",
        "outputId": "ce013a51-0273-4b8a-d04f-1b441ba8731c"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "['coldcalling', 'teamwork', 'harwork', 'creativity', 'Collaboration', 'communication', 'accounting', 'sales', 'marketing', 'dataentry', 'Leadership skills', 'Adaptability and flexibility', 'Problem-solving', 'Decision-making', 'Creativity', 'Team-working', 'TimeManagement', 'Willingness-to-learn', nan, nan, nan, nan]\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "###  Function to extract Technical skillset\n",
        "def extract_tech_skill_set(input_text):\n",
        "    stop_words = set(nltk.corpus.stopwords.words('english'))\n",
        "    word_tokens = nltk.tokenize.word_tokenize(input_text)\n",
        "\n",
        "    # remove the stop words\n",
        "    filtrd_tokens = [w for w in word_tokens if w not in stop_words]\n",
        "\n",
        "    # remove the punctuation\n",
        "    filtrd_tokens = [w for w in word_tokens if w.isalpha()]\n",
        "\n",
        "    # generate bigrams and trigrams (such as artificial intelligence)\n",
        "    bigrams_trigrams = list(map(' '.join, nltk.everygrams(filtrd_tokens, 2, 3)))\n",
        "\n",
        "    # we create a set to keep the results in.\n",
        "    get_tech_skill_set = set()\n",
        "    \n",
        "    # we search for each token in our skills database\n",
        "    for token in filtrd_tokens:\n",
        "        if token.lower() in Tech_Skill:\n",
        "            get_tech_skill_set.add(token)\n",
        "    \n",
        "    # we search for each bigram and trigram in our skills database\n",
        "    for ngram in bigrams_trigrams:\n",
        "        if ngram.lower() in Tech_Skill:\n",
        "            get_tech_skill_set.add(ngram)\n",
        "    \n",
        "    return get_tech_skill_set"
      ],
      "metadata": {
        "id": "8aXM2pLvWHHV"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "###  Function to extract Non-Technical skillset \n",
        "def extract_nontech_skill_set(input_text):\n",
        "    stop_words = set(nltk.corpus.stopwords.words('english'))\n",
        "    word_tokens = nltk.tokenize.word_tokenize(input_text)\n",
        "\n",
        "    # remove the stop words\n",
        "    filtrd_tokens = [w for w in word_tokens if w not in stop_words]\n",
        "\n",
        "    # remove the punctuation\n",
        "    filtrd_tokens = [w for w in word_tokens if w.isalpha()]\n",
        "\n",
        "    # generate bigrams and trigrams (such as artificial intelligence)\n",
        "    bigrams_trigrams = list(map(' '.join, nltk.everygrams(filtrd_tokens, 2, 3)))\n",
        "\n",
        "    # we create a set to keep the results in.\n",
        "    get_nontech_skill_set = set()\n",
        "\n",
        "    # we search for each token in our skills database\n",
        "    for token in filtrd_tokens:\n",
        "      if token.lower() in Non_Tech_Skill:\n",
        "        get_nontech_skill_set.add(token)\n",
        "\n",
        "    # we search for each bigram and trigram in our skills database\n",
        "    for ngram in bigrams_trigrams:\n",
        "        if ngram.lower() in Non_Tech_Skill:\n",
        "            get_nontech_skill_set.add(ngram)\n",
        "\n",
        "    return  get_nontech_skill_set"
      ],
      "metadata": {
        "id": "ulBysLUxWY7U"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "### **Function to print Technical & Non-Technical Skills from resume, and csv data provided**"
      ],
      "metadata": {
        "id": "oInHCP-UXA1F"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "\n",
        "if __name__ == '__main__':\n",
        "    tech_skill_reqd = extract_tech_skill_set(text)\n",
        "    print(\"Technical_skills:\", tech_skill_reqd )\n",
        "    nontech_skills_ignored = extract_nontech_skill_set(text)\n",
        "    print(\"NON-Technical_skills:\", nontech_skills_ignored )"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "vuLlPhfIMUGZ",
        "outputId": "6c5cf65c-7edc-4782-ee60-fda15cb1a387"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Technical_skills: {'Php', 'Java', 'MySQL', 'Github', 'CSS', 'newspaper', 'Framework', 'framework', 'C', 'github', 'Visual', 'PostgreSQL'}\n",
            "NON-Technical_skills: set()\n"
          ]
        }
      ]
    }
  ]
}